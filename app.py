# app.py (Vers√£o de Diagn√≥stico)
import streamlit as st
import os
import google.generativeai as genai
from dotenv import load_dotenv

# Imports para LangChain
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- CONFIGURA√á√ÉO INICIAL DA P√ÅGINA E VARI√ÅVEIS DE AMBIENTE ---
st.set_page_config(page_title="Assistente Agrofel (Debug)", page_icon="üêû", layout="wide")

# L√≥gica para carregar a chave de API de forma segura
load_dotenv()
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
    except (FileNotFoundError, KeyError):
        st.error("Chave de API do Google n√£o encontrada! Configure-a no arquivo .env ou nos segredos do Streamlit.")
        st.stop()
genai.configure(api_key=api_key)


# --- FUN√á√ïES DE L√ìGICA (BACKEND DA APLICA√á√ÉO) ---

@st.cache_resource(show_spinner="A carregar/criar a base de conhecimento...")
def carregar_base_conhecimento():
    """
    Carrega ou cria o √≠ndice FAISS. Retorna o DB, o LLM e metadados de diagn√≥stico.
    """
    CAMINHO_INDEX_FAISS = "faiss_index_agrofel"
    PASTA_BULAS = "documentos/"
    
    diagnostico = {
        "arquivos_encontrados": [],
        "arquivos_falharam": [],
        "total_chunks": 0
    }

    if not os.path.exists(PASTA_BULAS) or not os.listdir(PASTA_BULAS):
        st.error(f"ERRO CR√çTICO: A pasta '{PASTA_BULAS}' n√£o foi encontrada ou est√° vazia no reposit√≥rio.")
        return None, None, diagnostico
    
    diagnostico["arquivos_encontrados"] = sorted([f for f in os.listdir(PASTA_BULAS) if f.lower().endswith('.pdf')])

    if os.path.exists(CAMINHO_INDEX_FAISS):
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        db = FAISS.load_local(CAMINHO_INDEX_FAISS, embeddings, allow_dangerous_deserialization=True)
    else:
        todos_documentos = []
        for nome_arquivo in diagnostico["arquivos_encontrados"]:
            try:
                caminho_completo = os.path.join(PASTA_BULAS, nome_arquivo)
                loader = PyPDFLoader(caminho_completo)
                paginas = loader.load()
                for pagina in paginas:
                    pagina.metadata['source'] = nome_arquivo
                todos_documentos.extend(paginas)
            except Exception as e:
                diagnostico["arquivos_falharam"].append(f"{nome_arquivo} (Erro: {e})")

        if not todos_documentos:
            st.error("Nenhum documento p√¥de ser processado. A base de dados n√£o pode ser criada.")
            return None, None, diagnostico

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
        chunks = text_splitter.split_documents(todos_documentos)
        diagnostico["total_chunks"] = len(chunks)
        
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        db = FAISS.from_documents(chunks, embeddings)
        db.save_local(CAMINHO_INDEX_FAISS)

    llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro-latest", temperature=0.4)
    return db, llm, diagnostico

def agente_especialista_recomenda(query: str, db, llm):
    # Esta fun√ß√£o permanece a mesma da vers√£o anterior
    if db is None or llm is None: return "Erro: A base de conhecimento n√£o est√° carregada."
    docs_relevantes = db.similarity_search(query, k=10)
    if not docs_relevantes: return "NAO_ENCONTRADO"
    contexto_bruto = "\n\n---\n\n".join([f"Chunk {i+1}:\n{doc.page_content}" for i, doc in enumerate(docs_relevantes)])
    prompt_filtragem = f"""Analise a PERGUNTA do usu√°rio e os seguintes CHUNKS de texto. Sua tarefa √© retornar APENAS o conte√∫do dos 3 chunks que s√£o MAIS RELEVANTES para responder √† pergunta, separados por '---'.\n\nPERGUNTA: "{query}"\n\nCHUNKS:\n{contexto_bruto}"""
    llm_filtro = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.0)
    contexto_filtrado = llm_filtro.invoke(prompt_filtragem).content
    prompt_geracao_final = f"""Voc√™ √© um consultor especialista da Agrofel. Com base nos TRECHOS RELEVANTES DAS BULAS, gere uma recomenda√ß√£o.\n\nPERGUNTA DO AGRICULTOR: "{query}"\n\nTRECHOS RELEVANTES DAS BULAS:\n---\n{contexto_filtrado}\n---\nINSTRU√á√ïES:\n1. Sugira at√© DOIS produtos. Se encontrar apenas um, sugira apenas um.\n2. Extraia o nome exato do produto e crie uma descri√ß√£o curta.\n3. Formato:\n   **Produto 1:** [Nome do Produto]\n   **Descri√ß√£o:** [Sua descri√ß√£o]\n4. Se os trechos n√£o forem suficientes, responda APENAS com: "NAO_ENCONTRADO"."""
    resposta_final = llm.invoke(prompt_geracao_final)
    return resposta_final.content

# --- INTERFACE DO USU√ÅRIO (STREAMLIT) ---

st.title("üåø Assistente de Campo Agrofel")
st.markdown("Bem-vindo! Descreva seu problema com pragas na lavoura e encontrarei a melhor solu√ß√£o para voc√™.")

db, llm, info_diagnostico = carregar_base_conhecimento()

# --- PAINEL DE DIAGN√ìSTICO ---
with st.expander("üêû Informa√ß√µes de Diagn√≥stico (Ambiente de Produ√ß√£o)"):
    st.write("**Ficheiros PDF Encontrados no Reposit√≥rio:**", len(info_diagnostico["arquivos_encontrados"]))
    st.json(info_diagnostico["arquivos_encontrados"])
    
    if info_diagnostico["arquivos_falharam"]:
        st.error("**Ficheiros que Falharam ao Processar:**", len(info_diagnostico["arquivos_falharam"]))
        st.json(info_diagnostico["arquivos_falharam"])
    else:
        st.success("Todos os ficheiros foram processados com sucesso.")

    if info_diagnostico["total_chunks"] > 0:
        st.write("**Total de Chunks de Texto Criados:**", info_diagnostico["total_chunks"])

    st.info("Use o bot√£o abaixo para testar a busca pela consulta espec√≠fica que est√° a falhar.")
    if st.button("Executar Teste de Diagn√≥stico para 'Capim Amargoso na Soja'"):
        with st.spinner("A executar teste..."):
            query_teste = "Qual produto usar para Capim-amargoso na cultura da soja?"
            docs_teste = db.similarity_search_with_score(query_teste, k=5)
            st.subheader("Resultado da Busca por Similaridade (Etapa 1)")
            if not docs_teste:
                st.error("A busca n√£o retornou NENHUM resultado.")
            else:
                for i, (doc, score) in enumerate(docs_teste):
                    st.write(f"**Chunk {i+1} (Score: {score:.4f})**")
                    st.write(f"Fonte: `{doc.metadata.get('source', 'N/D')}`")
                    st.code(doc.page_content, language="text")

# --- L√≥gica Principal da Aplica√ß√£o ---
# (O restante do c√≥digo permanece o mesmo)
if 'recomendacao' not in st.session_state: st.session_state.recomendacao = ""
if 'pergunta' not in st.session_state: st.session_state.pergunta = ""

with st.form("pergunta_form"):
    pergunta_usuario = st.text_area("Qual praga est√° afetando sua lavoura e em qual cultura?", height=100, placeholder="Ex: Lagarta do cartucho no milho")
    submitted = st.form_submit_button("Buscar Sugest√µes")

if submitted and pergunta_usuario:
    if db is not None:
        with st.spinner("Analisando as melhores solu√ß√µes para voc√™..."):
            st.session_state.pergunta = pergunta_usuario
            recomendacao_gerada = agente_especialista_recomenda(pergunta_usuario, db, llm)
            st.session_state.recomendacao = recomendacao_gerada
    else:
        st.error("A base de conhecimento n√£o p√¥de ser carregada.")

if st.session_state.recomendacao:
    # (A l√≥gica de exibi√ß√£o da resposta permanece a mesma)
    if "NAO_ENCONTRADO" in st.session_state.recomendacao:
        st.warning("N√£o encontrei produtos espec√≠ficos para sua solicita√ß√£o em nossa base de dados.")
    else:
        st.subheader("Encontrei estas sugest√µes para voc√™:")
        st.markdown(st.session_state.recomendacao)
