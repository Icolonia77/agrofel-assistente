# app.py
import streamlit as st
import os
import google.generativeai as genai
from dotenv import load_dotenv
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS

# --- CONFIGURA√á√ÉO INICIAL DA P√ÅGINA E VARI√ÅVEIS DE AMBIENTE ---
st.set_page_config(page_title="Assistente Agrofel", page_icon="üåø", layout="wide")

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    # Exibe um erro na interface do Streamlit se a chave n√£o for encontrada
    st.error("Chave de API do Google n√£o encontrada! Verifique seu arquivo .env")
    st.stop()
genai.configure(api_key=api_key)

# Caminho completo para onde o √≠ndice FAISS est√° salvo no seu ambiente local
CAMINHO_INDEX_FAISS = "C:/Users/Usuario/Documents/Agrofel/faiss_index_agrofel"

# --- FUN√á√ïES DE L√ìGICA (BACKEND DA APLICA√á√ÉO) ---

@st.cache_resource
def carregar_base_conhecimento():
    """
    Carrega o √≠ndice FAISS do disco e inicializa o modelo de linguagem.
    Usa o cache do Streamlit para n√£o recarregar a cada intera√ß√£o do usu√°rio.
    """
    if not os.path.exists(CAMINHO_INDEX_FAISS):
        st.error(f"A base de conhecimento (√≠ndice FAISS) n√£o foi encontrada em '{CAMINHO_INDEX_FAISS}'. Execute o script '1_Criar_Base_Vetorial.py' primeiro.")
        return None, None
    
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        db = FAISS.load_local(CAMINHO_INDEX_FAISS, embeddings, allow_dangerous_deserialization=True)
        # Usamos um modelo mais capaz para a gera√ß√£o final
        llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro-latest", temperature=0.4)
        return db, llm
    except Exception as e:
        st.error(f"Ocorreu um erro ao carregar a base de conhecimento: {e}")
        return None, None

def agente_especialista_recomenda(query: str, db, llm):
    """
    Usa uma l√≥gica RAG de m√∫ltiplas etapas para encontrar e recomendar produtos.
    1. Busca inicial ampla para capturar qualquer informa√ß√£o relevante.
    2. Filtragem e Re-rankeamento com um LLM para limpar os dados.
    3. Gera√ß√£o da resposta final com um LLM mais robusto.
    """
    if db is None or llm is None:
        return "Erro: A base de conhecimento n√£o est√° carregada."

    # ETAPA 1: BUSCA VETORIAL AMPLA
    # Buscamos mais documentos (k=10) para aumentar a chance de encontrar algo relevante, mesmo que "sujo".
    docs_relevantes = db.similarity_search(query, k=10)
    
    if not docs_relevantes:
        return "NAO_ENCONTRADO"

    # ETAPA 2: FILTRAGEM E RE-RANKEAMENTO COM LLM
    # Criamos um contexto inicial com todos os chunks encontrados para o LLM limpar.
    contexto_bruto = "\n\n---\n\n".join([f"Chunk {i+1}:\n{doc.page_content}" for i, doc in enumerate(docs_relevantes)])

    # Prompt para o LLM atuar como um filtro inteligente.
    prompt_filtragem = f"""
    Analise a PERGUNTA do usu√°rio e os seguintes CHUNKS de texto extra√≠dos de bulas de produtos.
    Sua tarefa √© identificar e retornar APENAS o conte√∫do dos 3 chunks que s√£o MAIS RELEVANTES e √öTEIS para responder √† pergunta.
    N√£o adicione nenhuma palavra sua, apenas retorne o texto exato dos chunks selecionados, separados por '---'.

    PERGUNTA: "{query}"

    CHUNKS:
    {contexto_bruto}
    """
    
    # Usamos um modelo r√°pido e eficiente para a tarefa de filtragem.
    llm_filtro = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.0)
    contexto_filtrado = llm_filtro.invoke(prompt_filtragem).content

    # ETAPA 3: GERA√á√ÉO DA RESPOSTA FINAL
    # Agora usamos o contexto limpo e filtrado para gerar a resposta final ao usu√°rio.
    prompt_geracao_final = f"""
    Voc√™ √© um consultor especialista da Agrofel. Sua tarefa √© analisar a pergunta de um agricultor
    e, com base nos TRECHOS RELEVANTES DAS BULAS, gerar uma recomenda√ß√£o clara e objetiva.

    PERGUNTA DO AGRICULTOR: "{query}"

    TRECHOS RELEVANTES DAS BULAS (j√° pr√©-filtrados):
    ---
    {contexto_filtrado}
    ---

    INSTRU√á√ïES:
    1. Com base nos trechos, identifique e sugira at√© DOIS melhores produtos. Se encontrar apenas um, sugira apenas um.
    2. Para cada produto, extraia o nome exato e crie uma descri√ß√£o curta e convincente, mencionando a praga/cultura.
    3. Apresente a resposta no seguinte formato, e somente neste formato:
       
       **Produto 1:** [Nome do Produto]
       **Descri√ß√£o:** [Sua descri√ß√£o persuasiva aqui]

       **Produto 2:** [Nome do Produto]
       **Descri√ß√£o:** [Sua descri√ß√£o persuasiva aqui]
    
    4. Se mesmo ap√≥s a filtragem os trechos n√£o forem suficientes para uma recomenda√ß√£o clara e segura, responda APENAS com: "NAO_ENCONTRADO".
    """

    # Usamos o modelo principal para a gera√ß√£o da resposta final de alta qualidade.
    resposta_final = llm.invoke(prompt_geracao_final)
    return resposta_final.content

def notificar_vendedor(pergunta, recomendacao):
    """Simula o acionamento do vendedor."""
    print("\n--- [A√á√ÉO INTERNA: LEAD ENVIADO PARA VENDAS] ---")
    print(f"Cliente fez a pergunta: '{pergunta}'")
    print("Produtos sugeridos e aceitos:")
    print(recomendacao)
    print("-------------------------------------------------\n")
    # Em produ√ß√£o: aqui iria o c√≥digo para enviar email, chamar uma API do CRM, etc.

def encaminhar_para_humano(pergunta):
    """Simula o encaminhamento para atendimento humano."""
    print("\n--- [A√á√ÉO INTERNA: ATENDIMENTO TRANSFERIDO] ---")
    print(f"Cliente com a pergunta '{pergunta}' solicitou falar com um especialista.")
    print("-------------------------------------------------\n")
    # Em produ√ß√£o: aqui iria o c√≥digo para criar um ticket em um sistema de suporte.

# --- INTERFACE DO USU√ÅRIO (STREAMLIT) ---

st.title("üåø Assistente de Campo Agrofel")
st.markdown("Bem-vindo! Descreva seu problema com pragas na lavoura e encontrarei a melhor solu√ß√£o para voc√™.")

# Carrega a base de conhecimento uma vez e a mant√©m em cache.
db, llm = carregar_base_conhecimento()

# Inicializa o estado da sess√£o para controlar o fluxo da conversa.
if 'recomendacao' not in st.session_state:
    st.session_state.recomendacao = ""
if 'pergunta' not in st.session_state:
    st.session_state.pergunta = ""

# Formul√°rio de pergunta para evitar que a p√°gina recarregue a cada letra digitada.
with st.form("pergunta_form"):
    pergunta_usuario = st.text_area("Qual praga est√° afetando sua lavoura e em qual cultura?", height=100, placeholder="Ex: Lagarta do cartucho no milho")
    submitted = st.form_submit_button("Buscar Sugest√µes")

if submitted and pergunta_usuario:
    # Apenas executa a busca se a base de dados foi carregada corretamente.
    if db is not None:
        with st.spinner("Analisando as melhores solu√ß√µes para voc√™..."):
            st.session_state.pergunta = pergunta_usuario
            recomendacao_gerada = agente_especialista_recomenda(pergunta_usuario, db, llm)
            st.session_state.recomendacao = recomendacao_gerada
    else:
        # Mensagem de erro se a base n√£o p√¥de ser carregada.
        st.error("A base de conhecimento n√£o p√¥de ser carregada. Verifique os logs do terminal.")

# L√≥gica de Exibi√ß√£o dos Resultados e A√ß√µes
if st.session_state.recomendacao:
    if "NAO_ENCONTRADO" in st.session_state.recomendacao:
        st.warning("N√£o encontrei produtos espec√≠ficos para sua solicita√ß√£o em nossa base de dados.")
        st.info("Gostaria de falar com um de nossos consultores para obter ajuda personalizada?")
        if st.button("Sim, solicitar contato de um especialista"):
            encaminhar_para_humano(st.session_state.pergunta)
            st.success("Sua solicita√ß√£o foi enviada! Um consultor Agrofel entrar√° em contato em breve.")
            st.session_state.recomendacao = "" # Limpa o estado para uma nova consulta
    else:
        st.subheader("Encontrei estas sugest√µes para voc√™:")
        # Usamos st.markdown para renderizar a formata√ß√£o em negrito.
        st.markdown(st.session_state.recomendacao)
        
        st.markdown("---")
        st.markdown("O que voc√™ gostaria de fazer?")

        col1, col2 = st.columns(2)

        with col1:
            if st.button("‚úÖ Confirmar Pedido", type="primary", use_container_width=True):
                notificar_vendedor(st.session_state.pergunta, st.session_state.recomendacao)
                st.success("Pedido confirmado! Seu consultor Agrofel foi notificado e dar√° andamento √† sua solicita√ß√£o.")
                st.balloons()
                st.session_state.recomendacao = "" # Limpa o estado
        
        with col2:
            if st.button("üó£Ô∏è Falar com um Humano", use_container_width=True):
                encaminhar_para_humano(st.session_state.pergunta)
                st.info("Sua solicita√ß√£o foi registrada. Um especialista entrar√° em contato para discutir outras op√ß√µes.")
                st.session_state.recomendacao = "" # Limpa o estado
